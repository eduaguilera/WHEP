---
title: "Follow the workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Follow the workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Git intro

## Cloning the repository

Assuming you are on Windows, install Git from [here](https://git-scm.com/downloads/win)
(if you don't know whether it's the 32 or 64 bits version, you most likely need the
64 bit one). You should have something called 'Git Bash' installed. You can open
Git Bash inside a specific folder by right-clicking your folder and selecting
'Open Git Bash here', or you may want to learn some basic commands to navigate
from the command line itself (from now on, __writing `<some-text>` is not part
of the command__, I just use it as a __placeholder__ for what you need to write there):

- Print your current directory:
```
pwd
```
- List files from your current directory:
```
ls
```
- Move to another directory relative to the one you are in right now:
```
cd <relative-path-where-to-move>
```

We assume here that the repository you want to contribute to already exists.
You can go to its page in Github and copy the URL as seen in the image below:

```{r, fig.alt="Where to copy URL from", echo=FALSE}
knitr::include_graphics("imgs/copy_repo_url.png")
```

The git terminology used for 'downloading' a repository to our local file system
is 'cloning'. We can clone a remote repository (in this case from Github)
using the following command:
```
git clone <url-you-copied>
```

This is called cloning via HTTPS. You should be asked to introduce your Github
credentials. You can also clone via SSH which is more secure in the sense that
it won't ask you to sign in, but if you know that you probably don't need this
guide.

## Pulling remote changes

Now a new directory should have been created with the content of the repository
in your local file system. From now on we will see the basic git commands that
you would need in daily usage. We assume you are inside the repository. We
explain them with an example.

Suppose you want to start contributing to this repository. A good practice
(and one that we will enforce to use) is to make your own code changes in a
'different place' than the ones you currently see in the repository. The things
you see now are in what it's called the 'main branch', and you will make your
code changes in a 'new branch', which will start with the same content as the
main one, but will then evolve with different changes. If you haven't done
anything yet, you should be in the main branch (maybe it's called 'main' or
'master', these are just conventions, but I will assume it's called 'main').
You can use the command `git status` to check this (don't mind that my terminal
looks different in the screenshots, you can use the same commands in Git Bash):

```{r, fig.alt="Using git status", echo=FALSE}
knitr::include_graphics("imgs/git_status_command.png")
```

Your local version of a repository does not need to match the remote version
(the one we store in Github in this case), but before you start your work on
a new branch, you should keep your main branch up to date in case someone
added new code in the Github repository since the last time you checked. We
get any new remote changes to the local repository by using the command
```
git pull
```

```{r, fig.alt="Using git pull outputs 'Already up to date'", echo=FALSE}
knitr::include_graphics("imgs/git_pull_command.png")
```

In this case I already had all the remote changes, and that's why the message
says 'Already up to date', but the message will be different if you had missing
changes. This is the 'easy way' to do it. The command `git pull` tries to
fetch changes from the equivalent remote branch, i.e., the one that has the same
name on remote as it has on your local repository. This may not always work as
expected so there is a way to always specify from which remote branch you
want to get these changes (and I highly recommend always using it explicitly):
```
git pull origin <name-of-remote-branch>
```
For example, imagine you asked someone for help on your own branch and they
added some new changes on your branch, that you do not have locally. Then,
if your branch is called `my-branch`, and you are already on your branch
locally, you would want to use the command
```
git pull origin my-branch
```
Likewise, for the first example shown here (keeping the main branch updated),
I would always be explicit:
```
git pull origin main
```

## Creating our own branch

After the pull, we are now safely up to date with the remote changes. Now it's
time to learn how to create our own 'branch', from which we will start working
on new code. We use the following command:
```
git checkout -b <name-of-branch>
```

```{r, fig.alt="Using branch creation command", echo=FALSE}
knitr::include_graphics("imgs/branch_creation_command.png")
```

The command `git checkout <name-of-branch>` is used to change from one branch
to another (so that you will now see the files and changes that are in that
branch). Additionally, if we add the `-b` option, it will create the branch with
the given name if it doesn't already exist, which is our case in this example.
The branch name should be something like `author/name-of-branch`. Thus, some
common practices for naming your branches (and that we should follow) are:

- They do not contain caps (all lowercase)
- Words are separated with dashes (`-`)
- The name includes the author and some descriptive name separated by a slash (`/`)
- The descriptive name should ideally start with an action (a verb) in imperative
style (fix, create, test...).

If Ermenegildo wants to create some code for preprocessing bilateral trade data,
an acceptable branch name could be `ermenegildo/preprocess-bilateral-trade-data`.

## Adding changes to our branch

Now you are in your own branch and you can start working on your changes. While
you work on them, you should keep track of changes with git. We can add all
changes using the command
```
git add .
```
Here the dot means 'this directory', which essentially adds all new changes, i.e.
all things inside the directory. We can add just a specific file instead using
the command
```
git add <relative-name-of-file>
```

```{r, fig.alt="Add all git changes", echo=FALSE}
knitr::include_graphics("imgs/add_all_git_changes.png")
```

After adding our changes, we must 'commit' them. This commit step is what actually
saves your changes in the git history. You do this with the command
```
git commit -m 'Some descriptive message for your changes'
```
A common practice for commit messages is to start them with a verb in infinitive
(imperative style), indicating an action that was performed, e.g.,
`'Create tests for bilateral trade data preprocessing'`.

```{r, fig.alt="Commit the changes", echo=FALSE}
knitr::include_graphics("imgs/commit_changes.png")
```

A common practice is to make small commits, that is, include just a few changes
in each commit, so that it is easier to keep track of your work's history, instead
of just having a single commit when you are done with everything. Ultimately, the
amount of commits is your decision, but should not be just one commit per branch.

## Pushing our changes

After committing, we now have our changes in local git history, but we should
probably also add them to the remote Github repository. We do this using the command
```
git push origin <name-of-branch>
```
Now you should be able to see your changes in your own branch from Github itself,
you just need to select your own branch instead of the `main` one.

You should remember to push your changes regularly to the remote repository.
Otherwise you risk having a bunch of code features in your local computer that
could be lost if something happened to it. This is aligned with the previous
suggestion of creating many smaller commits as opposed to giant ones, so that
you can also push them more frequently.

## Creating a pull request

Suppose you are done with your changes and you want to add these to the main
branch. Mixing one branch with another is known as 'merging'. In this case we would
like to merge our new branch with the main branch. This can be done forcefully, but
the common practice we will be following is to create what is known as a 'Pull
request' from our branch into the main one, and we do this directly from Github,
once we have pushed all of our changes.

```{r, fig.alt="Click 'New pull request' in Github", echo=FALSE}
knitr::include_graphics("imgs/new_pull_request.png")
```

```{r, fig.alt="Click 'Create pull request'", echo=FALSE}
knitr::include_graphics("imgs/create_pull_request.png")
```

Here you can see all the changes you made (that differ from the main branch) before
clicking again 'Create pull request'. Then you will see the following, where you
should add some title and description to explain what you have done. You finally
click 'Create pull request' again.

```{r, fig.alt="Add title and description and click 'Create pull request'", echo=FALSE}
knitr::include_graphics("imgs/add_pr_title_description.png")
```

Now the Pull Request (often abbreviated as PR) is created and the next step is to
ask for someone's review.

```{r, fig.alt="Adding people to review your PR", echo=FALSE}
knitr::include_graphics("imgs/add_people_review_pr.png")
```

Ideally these changes would not be merged until someone else reviews your code.
This person might find things you have to change and request these changes before
merging, so you would have to keep working on your branch until they are satisfied.
Then they would accept your changes and you would be ready to merge your branch
into the main one, and the process would be done.

However, sometimes there is
an additional step that must be passed before merging, which is related to
automatic code checks, e.g. check whether your code is well formatted and
whether it passes all tests successfully. If configured, these can run
automatically when creating a Pull Request. We will indeed work with them, but we
will explain these automatic checks better in a later section.

While working on your own branch, others may have merged their own branches
into the main branch and then your own branch would be outdated. When creating
a Pull Request yourself, you should make sure your branch is also up to date with
everything already on the main branch. Recall from
[the pulling remote changes section](#pulling-remote-changes) that we can do this
with the command
```
git pull origin main
```
Even if you are locally on your own branch and directly try to fetch changes from
a different remote one (in this case `main`), this works as expected, that is,
it tries to merge all new changes from the `main` branch into your own local one.
This automatic merge works most of the times, but sometimes you may find conflicts,
because the program doesn't know how to combine everything neatly. If this happens,
you must manually check which parts of the code should be kept. We won't talk more
about this here since it's just an introduction. Whenever this happens to you,
you can ask others for help.


# R package and renv intro

## Project structure

It seems clear that even though we would work fine with bare R scripts that are run
directly, when working on a large project it makes sense to have some kind of
file structure, to keep everything organised. You can build your ad-hoc file
structures, and you could probably come up with something rather simple. Here,
instead, we will focus on using the standard structure of an R package. This
is a standard everyone has to follow if they want their projects to turn into
packages which can be publicly downloaded by anyone from the
[CRAN repositories](https://cran.r-project.org/). Just the same way you do, e.g.,
`install.packages(tidyverse)` to install all Tidyverse packages, if you
follow this standard R package structure, you can upload your package and one could do
`install.packages(your_package)` the same way.

Even if you don't want to upload a package, there are still advantages if you
follow this structure. This is the one we will follow, so the rest of this section
will try to explain its different parts, that will all become part of our workflow.

This is the whole structure of an R package:

```{r, fig.alt="Structure of an R package", echo=FALSE}
knitr::include_graphics("imgs/r_package_structure.png")
```

Luckily, there are a lot of files there that you do not need to know about,
at least for now, so we will try to explain the most important ones in the
next sections.

There is a whole [R packages book](https://r-pkgs.org/) which
I followed myself to setup the basics for our project. It is very well
written and available for free online, so if you are interested in knowing
more about R packages and their project structure, I recommend checking the
book.

## Virtual environments with renv

We just mentioned we were going to use the R package structure. And technically,
R package developers don't use `renv`... Then why should we? Well, the main
reason they don't use it is because when creating packages, you want to
make sure they work on fresh installations, i.e., computers that do not
have anything else installed, so this also excludes packages like `renv`.
However, if strictly necessary, it is possible to work with `renv` during
development phase and get rid of it in the final package sent publicly, and
there are enough advantages of using virtual environments that we should
try to benefit from.

But what are virtual environments? This is a fancy term, but its practical
meaning is quite simple. First consider the following:

- If you are not using them, it means you just have a global R installation
in your computer, and whenever you install a package, it is installed globally.
- If you want to run someone's code and they use a bunch of packages
that you usually do not, you would have to install all of them to be able
to run their code, and these would mix with all your other packages. If
you want to uninstall them after that, you would have to do a lot of manual
work to make sure you know all of them (some package dependencies could have
also been installed, and you cannot be sure if they were only used for these
packages or also some other package that you already had).
- If you want to write some code that uses some packages, and you want
another person to run it, you should make a list of the packages used only
in this project, because they should not have to install any other packages
you have from other projects but are not necessary here. If you do not
even make this 'package list', the other person should have to go through
your whole code or run it and install a new package every time the code
fails because of a missing one. Overall, this is a poor experience.

Virtual environments try to fix this. Essentially, they provide a 'local'
installation of packages, that are only visible inside your project, and
do not get mixed at all with those from your global R installation or from
other individual projects. In practice, a virtual environment is just a
folder containing installed packages, isolated from the folder that
contains your global R installation. It's like having several different
R installations, each one with their own packages and versions.

Chances are you follow this guide with an existing repository that is
already using `renv` (then you can skip the `renv::init()` step). If this
were not the case, open an R prompt in the root directory of your project
and run inside the prompt:
```
renv::init()
```
It will probably ask to close and reopen a clean prompt. After that, every
time we open an R prompt inside our project, it will automatically use `renv`
to work within a virtual environment. If you use `renv` for the first time
but on a project that already uses it, when you open the R prompt in its
root directory, the `renv` package will be installed automatically.

Now that we have `renv`, we can, for example, install a testing package
with `install.packages("testthat")` and this will not be a global
installation, which means it will only work inside this project. This is
a way of isolating your project dependencies and making your projects
reproducible, by letting others know exactly which packages your code
needs to run, and not add unnecessary ones that you may have because
of other projects, as we mentioned previously.

The 'list' of required packages for the project, along with their versions,
which is used by `renv` to manage the virtual environment, is in a file
called `renv.lock`. After installing new packages, this file is not updated
automatically and we have to do it manually by running
```
renv::snapshot()
```
This will update the `renv.lock` file, and you should push it to the
repository. If someone else wants to reproduce your code, they may have to run
```
renv::restore()
```
which will install any packages from `renv.lock` that they may still not
have installed, but again, only on a project level, not conflicting with
their global R installation. If you use Github with others, then you might
also need to do this every time you pull remote changes and someone else
has included a new package, so that you are then up to date with them.
In any case, when opening the R shell, it will probably remind you that there
are missing packages in your virtual environment with a message:

```{r, fig.alt="renv warns packages not installed", echo=FALSE}
knitr::include_graphics("imgs/renv_warn_packages_not_installed.png")
```

And this is basically all you need to start using a virtual environment,
keeping in mind the commands

- `renv::snapshot()`: add new required packages to `renv.lock` file
- `renv::restore()`: install packages from `renv.lock` that you do not have yet

I wrote this introduction to `renv` by reading their own package documentation.
If you want to learn more about it, you can read it yourself at
[their package website](https://rstudio.github.io/renv/articles/renv.html).

## Writing code

Looking back at the package's file structure, in the `R/` directory is
where we will put all the main code. The R files stored here must not
contain any top-level code, that is, it must all be inside functions.
We can add more than one function in each file if they are somehow related,
but there must not be too many either. If a file becomes too large and it
has several functions inside, consider splitting it into shorter files.

Take the following as an example (you do not have to understand the code,
you can keep reading). We save it in `R/sources.R`.
```r
#' Create a new dataframe where each row has a year range into one where each
#' row is a single year, effectively 'expanding' the whole year range
#'
#' @param trade_sources A tibble dataframe
#' where each row contains the year range
#'
#' @return A tibble dataframe where each row
#' corresponds to a single year for a given source
#'
#' @export
#'
#' @examples
#' trade_sources <- tibble::tibble(
#'   Name = c("a", "b", "c"),
#'   Trade = c("t1", "t2", "t3"),
#'   Info_Format = c("year", "partial_series", "year"),
#'   Timeline_Start = c(1, 1, 2),
#'   Timeline_End = c(3, 4, 5),
#'   Timeline_Freq = c(1, 1, 2),
#'   `Imp/Exp` = "Imp",
#'   SACO_link = NA,
#' )
#' expand_trade_sources(trade_sources)
expand_trade_sources <- function(trade_sources) {
  non_na_cols <- c("Trade", "Timeline_Start", "Timeline_End", "Timeline_Freq")
  trade_sources |>
    dplyr::filter(!.any_na_col(non_na_cols)) |>
    .expand_trade_years() |>
    dplyr::mutate(
      Name = dplyr::if_else(
        Info_Format == "year", paste(Name, Year, sep = "_"), Name
      ),
      ImpExp = `Imp/Exp`,
      In_Saco = as.integer(!is.na(SACO_link)),
    )
}

.expand_trade_years <- function(trade_sources) {
  trade_sources <- dplyr::mutate(trade_sources, No = dplyr::row_number())

  trade_sources |>
    dplyr::group_by(No) |>
    tidyr::expand(Year = seq(Timeline_Start, Timeline_End, Timeline_Freq)) |>
    dplyr::inner_join(trade_sources, by = "No")
}

.any_na_col <- function(cols_to_check) {
  dplyr::if_any(dplyr::all_of(cols_to_check), is.na)
}
```

In this sample code there are some things to keep in mind:

- All the code is written inside functions, and there are three of them. The
name of two of them starts with a dot. This is a convention for private functions.
Private functions are just helpers that are used in other functions from the
same file, they do not need to be used from outside.
- The functions that are not private, are then called public, and those are the ones
that we want to 'export', in the sense that we want to allow for them to be used
from outside this file. In our `sources.R` example, the first function is public.
- The public function has a large commented section before it, each line starting
with `#' `. This is a special type of comment and it is considered documentation.
Every public function __must__ be documented in the same way (more on this special
function documentation in the next section). The private functions can be
introduced by explanatory comments if you consider it necessary, but they should
be normal comments instead (starting with just `# `, without the single quote).

The most important take from here anyway is that these files should contain all
the code inside functions and nothing outside them.

## Function documentation

The special commented section seen in the previous example will be used by a
package called [`roxygen2`](https://roxygen2.r-lib.org/). We have to follow
this exact syntax, so that this package can automatically build a really neat
documentation of our package for us. Let's try to understand its basic
structure. For reference, these were the different parts:

- A small description of the function, nothing else.
```
#' Create a new dataframe where each row has a year range into one where each
#' row is a single year, effectively 'expanding' the whole year range
```

- A small description of each parameter the function receives. It should be like:
```
#' @param param_name_1 Description of param 1
#' @param param_name_2 Description of param 2
#' ...
```
As you see here I think it's OK to add linebreaks in between, as long as each
parameter starts with `@param`.
```
#' @param trade_sources A tibble dataframe
#' where each row contains the year range
```
- A small description of the value the function returns. It should start with
`@return`.
```r
#' @return A tibble dataframe where each row
#' corresponds to a single year for a given source
```
- A simple line containing `@export` to indicate the function can be used in
the package, i.e., it is public.
```r
#' @export
```

- A 'code' section of examples to illustrate the function's behaviour.
It must start with `@examples`, and after that you can write usual R code.
When this is processed, it automatically runs the code and adds some lines
with its output in the documentation.
```r
#' @examples
#' trade_sources <- tibble::tibble(
#'   Name = c("a", "b", "c"),
#'   Trade = c("t1", "t2", "t3"),
#'   Info_Format = c("year", "partial_series", "year"),
#'   Timeline_Start = c(1, 1, 2),
#'   Timeline_End = c(3, 4, 5),
#'   Timeline_Freq = c(1, 1, 2),
#'   `Imp/Exp` = "Imp",
#'   SACO_link = NA,
#' )
#' expand_trade_sources(trade_sources)
```

These options are enough to get us started with a nice documentation. In the
[Writing articles](#writing-articles) section we will learn how to generate
and see this documentation. In this example, it would look something like this
(note the autogenerated example code output):

```{r, fig.alt="Auto generated documentation appearance", echo=FALSE}
knitr::include_graphics("imgs/roxygen_documentation.png")
```

## Writing tests

## R CMD Check

## Writing articles

## R code style and formatting

There are some conventions and good practice for how to write neat code
in R (see e.g. [this](http://adv-r.had.co.nz/Style.html)). While it is nice to
know them, most of them can be automatically applied using some formatting tool.
There should be something like that in RStudio (I don't use it so I'm not sure)
or in other editors. It is good to try to make one of these autoformatters work
on your code because at some point I will try to add a formatting check on
Github in order not to allow pull requests being approved if they don't follow
these conventions, and it's way more tedious to try to check them manually.

## Automatic checks on Pull Requests
